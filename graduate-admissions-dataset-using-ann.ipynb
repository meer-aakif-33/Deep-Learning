{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":228180,"sourceType":"datasetVersion","datasetId":14872}],"dockerImageVersionId":29844,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Graduate Admissions Dataset\n\n\n\n## Context\n__This dataset is created for prediction of Graduate Admissions from an Indian perspective.__\n\n### Content\n\nThe dataset contains several parameters which are considered important during the application for Masters Programs. The parameters included are : \n\n1. GRE Scores ( out of 340 ) \n2. TOEFL Scores ( out of 120 ) \n3. University Rating ( out of 5 ) \n4. Statement of Purpose \n5. Letter of Recommendation Strength ( out of 5 )\n5. Undergraduate GPA ( out of 10 ) \n6. Research Experience ( either 0 or 1 ) \n7. Chance of Admit ( ranging from 0 to 1 )\n\n\n__Let load the necessary library in the Notebook__","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# from sklearn.linear_model import LinearRegression\n# from sklearn.model_selection import train_test_split\n# from sklearn.linear_model import RidgeCV\n# from sklearn.linear_model import LassoCV","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T03:34:22.231480Z","iopub.execute_input":"2025-08-24T03:34:22.231738Z","iopub.status.idle":"2025-08-24T03:34:25.025111Z","shell.execute_reply.started":"2025-08-24T03:34:22.231687Z","shell.execute_reply":"2025-08-24T03:34:25.023947Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"df = pd.read_csv('../input/graduate-admissions/Admission_Predict_Ver1.1.csv')\n\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T03:34:25.030362Z","iopub.execute_input":"2025-08-24T03:34:25.030752Z","iopub.status.idle":"2025-08-24T03:34:25.071360Z","shell.execute_reply.started":"2025-08-24T03:34:25.030685Z","shell.execute_reply":"2025-08-24T03:34:25.070372Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n0           1        337          118                  4  4.5   4.5  9.65   \n1           2        324          107                  4  4.0   4.5  8.87   \n2           3        316          104                  3  3.0   3.5  8.00   \n3           4        322          110                  3  3.5   2.5  8.67   \n4           5        314          103                  2  2.0   3.0  8.21   \n\n   Research  Chance of Admit   \n0         1              0.92  \n1         1              0.76  \n2         1              0.72  \n3         1              0.80  \n4         0              0.65  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Serial No.</th>\n      <th>GRE Score</th>\n      <th>TOEFL Score</th>\n      <th>University Rating</th>\n      <th>SOP</th>\n      <th>LOR</th>\n      <th>CGPA</th>\n      <th>Research</th>\n      <th>Chance of Admit</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>337</td>\n      <td>118</td>\n      <td>4</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>9.65</td>\n      <td>1</td>\n      <td>0.92</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>324</td>\n      <td>107</td>\n      <td>4</td>\n      <td>4.0</td>\n      <td>4.5</td>\n      <td>8.87</td>\n      <td>1</td>\n      <td>0.76</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>316</td>\n      <td>104</td>\n      <td>3</td>\n      <td>3.0</td>\n      <td>3.5</td>\n      <td>8.00</td>\n      <td>1</td>\n      <td>0.72</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>322</td>\n      <td>110</td>\n      <td>3</td>\n      <td>3.5</td>\n      <td>2.5</td>\n      <td>8.67</td>\n      <td>1</td>\n      <td>0.80</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>314</td>\n      <td>103</td>\n      <td>2</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>8.21</td>\n      <td>0</td>\n      <td>0.65</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T03:34:48.120172Z","iopub.execute_input":"2025-08-24T03:34:48.120578Z","iopub.status.idle":"2025-08-24T03:34:48.128129Z","shell.execute_reply.started":"2025-08-24T03:34:48.120517Z","shell.execute_reply":"2025-08-24T03:34:48.126783Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"(500, 9)"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"**Duplicted Data or not**","metadata":{}},{"cell_type":"code","source":"df.duplicated().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T03:35:56.253554Z","iopub.execute_input":"2025-08-24T03:35:56.253918Z","iopub.status.idle":"2025-08-24T03:35:56.266431Z","shell.execute_reply.started":"2025-08-24T03:35:56.253855Z","shell.execute_reply":"2025-08-24T03:35:56.265333Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T03:35:16.893651Z","iopub.execute_input":"2025-08-24T03:35:16.893989Z","iopub.status.idle":"2025-08-24T03:35:16.905215Z","shell.execute_reply.started":"2025-08-24T03:35:16.893933Z","shell.execute_reply":"2025-08-24T03:35:16.904256Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 500 entries, 0 to 499\nData columns (total 9 columns):\nSerial No.           500 non-null int64\nGRE Score            500 non-null int64\nTOEFL Score          500 non-null int64\nUniversity Rating    500 non-null int64\nSOP                  500 non-null float64\nLOR                  500 non-null float64\nCGPA                 500 non-null float64\nResearch             500 non-null int64\nChance of Admit      500 non-null float64\ndtypes: float64(4), int64(5)\nmemory usage: 35.3 KB\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.drop(columns=['Serial No.'], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T03:38:34.955520Z","iopub.execute_input":"2025-08-24T03:38:34.955862Z","iopub.status.idle":"2025-08-24T03:38:34.963996Z","shell.execute_reply.started":"2025-08-24T03:38:34.955812Z","shell.execute_reply":"2025-08-24T03:38:34.962903Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T03:38:42.951669Z","iopub.execute_input":"2025-08-24T03:38:42.952002Z","iopub.status.idle":"2025-08-24T03:38:42.995991Z","shell.execute_reply.started":"2025-08-24T03:38:42.951948Z","shell.execute_reply":"2025-08-24T03:38:42.995163Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"        GRE Score  TOEFL Score  University Rating         SOP       LOR   \\\ncount  500.000000   500.000000         500.000000  500.000000  500.00000   \nmean   316.472000   107.192000           3.114000    3.374000    3.48400   \nstd     11.295148     6.081868           1.143512    0.991004    0.92545   \nmin    290.000000    92.000000           1.000000    1.000000    1.00000   \n25%    308.000000   103.000000           2.000000    2.500000    3.00000   \n50%    317.000000   107.000000           3.000000    3.500000    3.50000   \n75%    325.000000   112.000000           4.000000    4.000000    4.00000   \nmax    340.000000   120.000000           5.000000    5.000000    5.00000   \n\n             CGPA    Research  Chance of Admit   \ncount  500.000000  500.000000         500.00000  \nmean     8.576440    0.560000           0.72174  \nstd      0.604813    0.496884           0.14114  \nmin      6.800000    0.000000           0.34000  \n25%      8.127500    0.000000           0.63000  \n50%      8.560000    1.000000           0.72000  \n75%      9.040000    1.000000           0.82000  \nmax      9.920000    1.000000           0.97000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>GRE Score</th>\n      <th>TOEFL Score</th>\n      <th>University Rating</th>\n      <th>SOP</th>\n      <th>LOR</th>\n      <th>CGPA</th>\n      <th>Research</th>\n      <th>Chance of Admit</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>500.000000</td>\n      <td>500.000000</td>\n      <td>500.000000</td>\n      <td>500.000000</td>\n      <td>500.00000</td>\n      <td>500.000000</td>\n      <td>500.000000</td>\n      <td>500.00000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>316.472000</td>\n      <td>107.192000</td>\n      <td>3.114000</td>\n      <td>3.374000</td>\n      <td>3.48400</td>\n      <td>8.576440</td>\n      <td>0.560000</td>\n      <td>0.72174</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>11.295148</td>\n      <td>6.081868</td>\n      <td>1.143512</td>\n      <td>0.991004</td>\n      <td>0.92545</td>\n      <td>0.604813</td>\n      <td>0.496884</td>\n      <td>0.14114</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>290.000000</td>\n      <td>92.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.00000</td>\n      <td>6.800000</td>\n      <td>0.000000</td>\n      <td>0.34000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>308.000000</td>\n      <td>103.000000</td>\n      <td>2.000000</td>\n      <td>2.500000</td>\n      <td>3.00000</td>\n      <td>8.127500</td>\n      <td>0.000000</td>\n      <td>0.63000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>317.000000</td>\n      <td>107.000000</td>\n      <td>3.000000</td>\n      <td>3.500000</td>\n      <td>3.50000</td>\n      <td>8.560000</td>\n      <td>1.000000</td>\n      <td>0.72000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>325.000000</td>\n      <td>112.000000</td>\n      <td>4.000000</td>\n      <td>4.000000</td>\n      <td>4.00000</td>\n      <td>9.040000</td>\n      <td>1.000000</td>\n      <td>0.82000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>340.000000</td>\n      <td>120.000000</td>\n      <td>5.000000</td>\n      <td>5.000000</td>\n      <td>5.00000</td>\n      <td>9.920000</td>\n      <td>1.000000</td>\n      <td>0.97000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T03:38:39.376707Z","iopub.execute_input":"2025-08-24T03:38:39.377056Z","iopub.status.idle":"2025-08-24T03:38:39.386810Z","shell.execute_reply.started":"2025-08-24T03:38:39.376991Z","shell.execute_reply":"2025-08-24T03:38:39.385914Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 500 entries, 0 to 499\nData columns (total 8 columns):\nGRE Score            500 non-null int64\nTOEFL Score          500 non-null int64\nUniversity Rating    500 non-null int64\nSOP                  500 non-null float64\nLOR                  500 non-null float64\nCGPA                 500 non-null float64\nResearch             500 non-null int64\nChance of Admit      500 non-null float64\ndtypes: float64(4), int64(4)\nmemory usage: 31.4 KB\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"X = df.iloc[:,0:-1]\ny = df.iloc[:,-1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T03:40:19.711070Z","iopub.execute_input":"2025-08-24T03:40:19.711402Z","iopub.status.idle":"2025-08-24T03:40:19.717394Z","shell.execute_reply.started":"2025-08-24T03:40:19.711353Z","shell.execute_reply":"2025-08-24T03:40:19.716353Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T03:40:25.613796Z","iopub.execute_input":"2025-08-24T03:40:25.614135Z","iopub.status.idle":"2025-08-24T03:40:25.633782Z","shell.execute_reply.started":"2025-08-24T03:40:25.614072Z","shell.execute_reply":"2025-08-24T03:40:25.632479Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"     GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research\n0          337          118                  4  4.5   4.5  9.65         1\n1          324          107                  4  4.0   4.5  8.87         1\n2          316          104                  3  3.0   3.5  8.00         1\n3          322          110                  3  3.5   2.5  8.67         1\n4          314          103                  2  2.0   3.0  8.21         0\n..         ...          ...                ...  ...   ...   ...       ...\n495        332          108                  5  4.5   4.0  9.02         1\n496        337          117                  5  5.0   5.0  9.87         1\n497        330          120                  5  4.5   5.0  9.56         1\n498        312          103                  4  4.0   5.0  8.43         0\n499        327          113                  4  4.5   4.5  9.04         0\n\n[500 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>GRE Score</th>\n      <th>TOEFL Score</th>\n      <th>University Rating</th>\n      <th>SOP</th>\n      <th>LOR</th>\n      <th>CGPA</th>\n      <th>Research</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>337</td>\n      <td>118</td>\n      <td>4</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>9.65</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>324</td>\n      <td>107</td>\n      <td>4</td>\n      <td>4.0</td>\n      <td>4.5</td>\n      <td>8.87</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>316</td>\n      <td>104</td>\n      <td>3</td>\n      <td>3.0</td>\n      <td>3.5</td>\n      <td>8.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>322</td>\n      <td>110</td>\n      <td>3</td>\n      <td>3.5</td>\n      <td>2.5</td>\n      <td>8.67</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>314</td>\n      <td>103</td>\n      <td>2</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>8.21</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>495</th>\n      <td>332</td>\n      <td>108</td>\n      <td>5</td>\n      <td>4.5</td>\n      <td>4.0</td>\n      <td>9.02</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>496</th>\n      <td>337</td>\n      <td>117</td>\n      <td>5</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>9.87</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>497</th>\n      <td>330</td>\n      <td>120</td>\n      <td>5</td>\n      <td>4.5</td>\n      <td>5.0</td>\n      <td>9.56</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>498</th>\n      <td>312</td>\n      <td>103</td>\n      <td>4</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>8.43</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>499</th>\n      <td>327</td>\n      <td>113</td>\n      <td>4</td>\n      <td>4.5</td>\n      <td>4.5</td>\n      <td>9.04</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>500 rows × 7 columns</p>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T03:40:33.465650Z","iopub.execute_input":"2025-08-24T03:40:33.466014Z","iopub.status.idle":"2025-08-24T03:40:33.473643Z","shell.execute_reply.started":"2025-08-24T03:40:33.465943Z","shell.execute_reply":"2025-08-24T03:40:33.472584Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"0      0.92\n1      0.76\n2      0.72\n3      0.80\n4      0.65\n       ... \n495    0.87\n496    0.96\n497    0.93\n498    0.73\n499    0.84\nName: Chance of Admit , Length: 500, dtype: float64"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=34)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T03:42:16.162392Z","iopub.execute_input":"2025-08-24T03:42:16.162782Z","iopub.status.idle":"2025-08-24T03:42:16.524402Z","shell.execute_reply.started":"2025-08-24T03:42:16.162717Z","shell.execute_reply":"2025-08-24T03:42:16.523487Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T03:44:06.362938Z","iopub.execute_input":"2025-08-24T03:44:06.363245Z","iopub.status.idle":"2025-08-24T03:44:06.372766Z","shell.execute_reply.started":"2025-08-24T03:44:06.363200Z","shell.execute_reply":"2025-08-24T03:44:06.371509Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"X_train_scaled","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T03:44:16.822216Z","iopub.execute_input":"2025-08-24T03:44:16.822582Z","iopub.status.idle":"2025-08-24T03:44:16.829398Z","shell.execute_reply.started":"2025-08-24T03:44:16.822514Z","shell.execute_reply":"2025-08-24T03:44:16.828157Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"array([[0.48      , 0.55555556, 0.5       , ..., 0.57142857, 0.43648208,\n        0.        ],\n       [0.22      , 0.25925926, 0.5       , ..., 0.42857143, 0.40390879,\n        0.        ],\n       [0.26      , 0.22222222, 0.5       , ..., 0.28571429, 0.28013029,\n        0.        ],\n       ...,\n       [0.34      , 0.44444444, 0.25      , ..., 0.85714286, 0.42996743,\n        1.        ],\n       [0.4       , 0.48148148, 0.75      , ..., 0.28571429, 0.50814332,\n        0.        ],\n       [0.52      , 0.37037037, 0.5       , ..., 0.14285714, 0.28664495,\n        0.        ]])"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"import tensorflow\nfrom tensorflow import keras\nfrom keras import Sequential\n\nfrom keras.layers import Dense","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T03:45:51.351459Z","iopub.execute_input":"2025-08-24T03:45:51.351818Z","iopub.status.idle":"2025-08-24T03:45:51.496842Z","shell.execute_reply.started":"2025-08-24T03:45:51.351751Z","shell.execute_reply":"2025-08-24T03:45:51.495896Z"}},"outputs":[{"name":"stderr","text":"Using TensorFlow backend.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Dense(7, activation='relu', input_dim=7))\nmodel.add(Dense(7, activation='relu'))\n#model.add(Dense(7, activation='relu'))\nmodel.add(Dense(1,activation='linear'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T03:56:35.979996Z","iopub.execute_input":"2025-08-24T03:56:35.980305Z","iopub.status.idle":"2025-08-24T03:56:36.008828Z","shell.execute_reply.started":"2025-08-24T03:56:35.980257Z","shell.execute_reply":"2025-08-24T03:56:36.007589Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T03:56:36.840665Z","iopub.execute_input":"2025-08-24T03:56:36.840985Z","iopub.status.idle":"2025-08-24T03:56:36.847707Z","shell.execute_reply.started":"2025-08-24T03:56:36.840935Z","shell.execute_reply":"2025-08-24T03:56:36.846592Z"}},"outputs":[{"name":"stdout","text":"Model: \"sequential_5\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_10 (Dense)             (None, 7)                 56        \n_________________________________________________________________\ndense_11 (Dense)             (None, 7)                 56        \n_________________________________________________________________\ndense_12 (Dense)             (None, 1)                 8         \n=================================================================\nTotal params: 120\nTrainable params: 120\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"model.compile(loss='mean_squared_error', optimizer='Adam')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T03:56:37.852195Z","iopub.execute_input":"2025-08-24T03:56:37.852518Z","iopub.status.idle":"2025-08-24T03:56:37.870607Z","shell.execute_reply.started":"2025-08-24T03:56:37.852467Z","shell.execute_reply":"2025-08-24T03:56:37.869401Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"history = model.fit(X_train_scaled, y_train, epochs=200, validation_split=0.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T03:56:39.359501Z","iopub.execute_input":"2025-08-24T03:56:39.359874Z","iopub.status.idle":"2025-08-24T03:56:43.365415Z","shell.execute_reply.started":"2025-08-24T03:56:39.359809Z","shell.execute_reply":"2025-08-24T03:56:43.364489Z"}},"outputs":[{"name":"stdout","text":"Train on 320 samples, validate on 80 samples\nEpoch 1/200\n320/320 [==============================] - 0s 426us/step - loss: 0.2904 - val_loss: 0.2423\nEpoch 2/200\n320/320 [==============================] - 0s 54us/step - loss: 0.2064 - val_loss: 0.1694\nEpoch 3/200\n320/320 [==============================] - 0s 51us/step - loss: 0.1400 - val_loss: 0.1124\nEpoch 4/200\n320/320 [==============================] - 0s 50us/step - loss: 0.0903 - val_loss: 0.0716\nEpoch 5/200\n320/320 [==============================] - 0s 53us/step - loss: 0.0558 - val_loss: 0.0442\nEpoch 6/200\n320/320 [==============================] - 0s 54us/step - loss: 0.0344 - val_loss: 0.0278\nEpoch 7/200\n320/320 [==============================] - 0s 50us/step - loss: 0.0231 - val_loss: 0.0186\nEpoch 8/200\n320/320 [==============================] - 0s 49us/step - loss: 0.0168 - val_loss: 0.0138\nEpoch 9/200\n320/320 [==============================] - 0s 53us/step - loss: 0.0135 - val_loss: 0.0119\nEpoch 10/200\n320/320 [==============================] - 0s 54us/step - loss: 0.0128 - val_loss: 0.0108\nEpoch 11/200\n320/320 [==============================] - 0s 54us/step - loss: 0.0115 - val_loss: 0.0098\nEpoch 12/200\n320/320 [==============================] - 0s 53us/step - loss: 0.0106 - val_loss: 0.0089\nEpoch 13/200\n320/320 [==============================] - 0s 51us/step - loss: 0.0097 - val_loss: 0.0081\nEpoch 14/200\n320/320 [==============================] - 0s 47us/step - loss: 0.0090 - val_loss: 0.0075\nEpoch 15/200\n320/320 [==============================] - 0s 49us/step - loss: 0.0084 - val_loss: 0.0069\nEpoch 16/200\n320/320 [==============================] - 0s 51us/step - loss: 0.0079 - val_loss: 0.0064\nEpoch 17/200\n320/320 [==============================] - 0s 45us/step - loss: 0.0074 - val_loss: 0.0059\nEpoch 18/200\n320/320 [==============================] - 0s 47us/step - loss: 0.0070 - val_loss: 0.0055\nEpoch 19/200\n320/320 [==============================] - 0s 50us/step - loss: 0.0066 - val_loss: 0.0052\nEpoch 20/200\n320/320 [==============================] - 0s 48us/step - loss: 0.0063 - val_loss: 0.0049\nEpoch 21/200\n320/320 [==============================] - 0s 46us/step - loss: 0.0061 - val_loss: 0.0047\nEpoch 22/200\n320/320 [==============================] - 0s 48us/step - loss: 0.0059 - val_loss: 0.0045\nEpoch 23/200\n320/320 [==============================] - 0s 54us/step - loss: 0.0057 - val_loss: 0.0044\nEpoch 24/200\n320/320 [==============================] - 0s 56us/step - loss: 0.0055 - val_loss: 0.0042\nEpoch 25/200\n320/320 [==============================] - 0s 52us/step - loss: 0.0054 - val_loss: 0.0042\nEpoch 26/200\n320/320 [==============================] - 0s 51us/step - loss: 0.0053 - val_loss: 0.0041\nEpoch 27/200\n320/320 [==============================] - 0s 53us/step - loss: 0.0052 - val_loss: 0.0040\nEpoch 28/200\n320/320 [==============================] - 0s 51us/step - loss: 0.0052 - val_loss: 0.0040\nEpoch 29/200\n320/320 [==============================] - 0s 53us/step - loss: 0.0051 - val_loss: 0.0039\nEpoch 30/200\n320/320 [==============================] - 0s 52us/step - loss: 0.0051 - val_loss: 0.0039\nEpoch 31/200\n320/320 [==============================] - 0s 49us/step - loss: 0.0050 - val_loss: 0.0039\nEpoch 32/200\n320/320 [==============================] - 0s 48us/step - loss: 0.0050 - val_loss: 0.0038\nEpoch 33/200\n320/320 [==============================] - 0s 47us/step - loss: 0.0050 - val_loss: 0.0038\nEpoch 34/200\n320/320 [==============================] - 0s 45us/step - loss: 0.0049 - val_loss: 0.0038\nEpoch 35/200\n320/320 [==============================] - 0s 53us/step - loss: 0.0049 - val_loss: 0.0038\nEpoch 36/200\n320/320 [==============================] - 0s 51us/step - loss: 0.0049 - val_loss: 0.0038\nEpoch 37/200\n320/320 [==============================] - 0s 48us/step - loss: 0.0048 - val_loss: 0.0038\nEpoch 38/200\n320/320 [==============================] - 0s 49us/step - loss: 0.0048 - val_loss: 0.0038\nEpoch 39/200\n320/320 [==============================] - 0s 50us/step - loss: 0.0048 - val_loss: 0.0038\nEpoch 40/200\n320/320 [==============================] - 0s 60us/step - loss: 0.0048 - val_loss: 0.0037\nEpoch 41/200\n320/320 [==============================] - 0s 54us/step - loss: 0.0048 - val_loss: 0.0038\nEpoch 42/200\n320/320 [==============================] - 0s 51us/step - loss: 0.0047 - val_loss: 0.0037\nEpoch 43/200\n320/320 [==============================] - 0s 55us/step - loss: 0.0047 - val_loss: 0.0037\nEpoch 44/200\n320/320 [==============================] - 0s 46us/step - loss: 0.0047 - val_loss: 0.0037\nEpoch 45/200\n320/320 [==============================] - 0s 49us/step - loss: 0.0047 - val_loss: 0.0037\nEpoch 46/200\n320/320 [==============================] - 0s 49us/step - loss: 0.0046 - val_loss: 0.0036\nEpoch 47/200\n320/320 [==============================] - 0s 51us/step - loss: 0.0046 - val_loss: 0.0036\nEpoch 48/200\n320/320 [==============================] - 0s 54us/step - loss: 0.0046 - val_loss: 0.0036\nEpoch 49/200\n320/320 [==============================] - 0s 50us/step - loss: 0.0046 - val_loss: 0.0036\nEpoch 50/200\n320/320 [==============================] - 0s 52us/step - loss: 0.0046 - val_loss: 0.0036\nEpoch 51/200\n320/320 [==============================] - 0s 49us/step - loss: 0.0046 - val_loss: 0.0035\nEpoch 52/200\n320/320 [==============================] - 0s 53us/step - loss: 0.0046 - val_loss: 0.0036\nEpoch 53/200\n320/320 [==============================] - 0s 48us/step - loss: 0.0046 - val_loss: 0.0035\nEpoch 54/200\n320/320 [==============================] - 0s 54us/step - loss: 0.0045 - val_loss: 0.0036\nEpoch 55/200\n320/320 [==============================] - 0s 52us/step - loss: 0.0045 - val_loss: 0.0035\nEpoch 56/200\n320/320 [==============================] - 0s 47us/step - loss: 0.0045 - val_loss: 0.0035\nEpoch 57/200\n320/320 [==============================] - 0s 50us/step - loss: 0.0045 - val_loss: 0.0035\nEpoch 58/200\n320/320 [==============================] - 0s 52us/step - loss: 0.0045 - val_loss: 0.0035\nEpoch 59/200\n320/320 [==============================] - 0s 49us/step - loss: 0.0044 - val_loss: 0.0035\nEpoch 60/200\n320/320 [==============================] - 0s 50us/step - loss: 0.0044 - val_loss: 0.0035\nEpoch 61/200\n320/320 [==============================] - 0s 52us/step - loss: 0.0044 - val_loss: 0.0034\nEpoch 62/200\n320/320 [==============================] - 0s 51us/step - loss: 0.0044 - val_loss: 0.0034\nEpoch 63/200\n320/320 [==============================] - 0s 49us/step - loss: 0.0044 - val_loss: 0.0034\nEpoch 64/200\n320/320 [==============================] - 0s 57us/step - loss: 0.0044 - val_loss: 0.0034\nEpoch 65/200\n320/320 [==============================] - 0s 53us/step - loss: 0.0044 - val_loss: 0.0034\nEpoch 66/200\n320/320 [==============================] - 0s 48us/step - loss: 0.0044 - val_loss: 0.0034\nEpoch 67/200\n320/320 [==============================] - 0s 48us/step - loss: 0.0044 - val_loss: 0.0033\nEpoch 68/200\n320/320 [==============================] - 0s 52us/step - loss: 0.0044 - val_loss: 0.0034\nEpoch 69/200\n320/320 [==============================] - 0s 48us/step - loss: 0.0044 - val_loss: 0.0033\nEpoch 70/200\n320/320 [==============================] - 0s 55us/step - loss: 0.0043 - val_loss: 0.0033\nEpoch 71/200\n320/320 [==============================] - 0s 49us/step - loss: 0.0043 - val_loss: 0.0033\nEpoch 72/200\n320/320 [==============================] - 0s 49us/step - loss: 0.0043 - val_loss: 0.0033\nEpoch 73/200\n320/320 [==============================] - 0s 48us/step - loss: 0.0043 - val_loss: 0.0033\nEpoch 74/200\n320/320 [==============================] - 0s 47us/step - loss: 0.0043 - val_loss: 0.0033\nEpoch 75/200\n320/320 [==============================] - 0s 49us/step - loss: 0.0043 - val_loss: 0.0033\nEpoch 76/200\n320/320 [==============================] - 0s 53us/step - loss: 0.0043 - val_loss: 0.0033\nEpoch 77/200\n320/320 [==============================] - 0s 52us/step - loss: 0.0043 - val_loss: 0.0033\nEpoch 78/200\n320/320 [==============================] - 0s 46us/step - loss: 0.0043 - val_loss: 0.0033\nEpoch 79/200\n320/320 [==============================] - 0s 46us/step - loss: 0.0043 - val_loss: 0.0032\nEpoch 80/200\n320/320 [==============================] - 0s 49us/step - loss: 0.0043 - val_loss: 0.0033\nEpoch 81/200\n320/320 [==============================] - 0s 51us/step - loss: 0.0042 - val_loss: 0.0032\nEpoch 82/200\n320/320 [==============================] - 0s 54us/step - loss: 0.0042 - val_loss: 0.0033\nEpoch 83/200\n320/320 [==============================] - 0s 48us/step - loss: 0.0042 - val_loss: 0.0032\nEpoch 84/200\n320/320 [==============================] - 0s 54us/step - loss: 0.0042 - val_loss: 0.0032\nEpoch 85/200\n320/320 [==============================] - 0s 52us/step - loss: 0.0042 - val_loss: 0.0032\nEpoch 86/200\n320/320 [==============================] - 0s 53us/step - loss: 0.0042 - val_loss: 0.0032\nEpoch 87/200\n320/320 [==============================] - 0s 52us/step - loss: 0.0042 - val_loss: 0.0032\nEpoch 88/200\n320/320 [==============================] - 0s 51us/step - loss: 0.0042 - val_loss: 0.0032\nEpoch 89/200\n320/320 [==============================] - 0s 54us/step - loss: 0.0042 - val_loss: 0.0032\nEpoch 90/200\n320/320 [==============================] - 0s 51us/step - loss: 0.0042 - val_loss: 0.0032\nEpoch 91/200\n320/320 [==============================] - 0s 52us/step - loss: 0.0042 - val_loss: 0.0032\nEpoch 92/200\n320/320 [==============================] - 0s 51us/step - loss: 0.0042 - val_loss: 0.0031\nEpoch 93/200\n320/320 [==============================] - 0s 48us/step - loss: 0.0042 - val_loss: 0.0032\nEpoch 94/200\n320/320 [==============================] - 0s 51us/step - loss: 0.0042 - val_loss: 0.0031\nEpoch 95/200\n320/320 [==============================] - 0s 50us/step - loss: 0.0042 - val_loss: 0.0031\nEpoch 96/200\n320/320 [==============================] - 0s 47us/step - loss: 0.0041 - val_loss: 0.0031\nEpoch 97/200\n320/320 [==============================] - 0s 49us/step - loss: 0.0041 - val_loss: 0.0031\nEpoch 98/200\n320/320 [==============================] - 0s 50us/step - loss: 0.0041 - val_loss: 0.0031\nEpoch 99/200\n320/320 [==============================] - 0s 52us/step - loss: 0.0041 - val_loss: 0.0031\nEpoch 100/200\n320/320 [==============================] - 0s 51us/step - loss: 0.0041 - val_loss: 0.0031\nEpoch 101/200\n320/320 [==============================] - 0s 51us/step - loss: 0.0041 - val_loss: 0.0031\nEpoch 102/200\n320/320 [==============================] - 0s 50us/step - loss: 0.0042 - val_loss: 0.0031\nEpoch 103/200\n320/320 [==============================] - 0s 50us/step - loss: 0.0041 - val_loss: 0.0031\nEpoch 104/200\n320/320 [==============================] - 0s 51us/step - loss: 0.0041 - val_loss: 0.0031\nEpoch 105/200\n320/320 [==============================] - 0s 55us/step - loss: 0.0041 - val_loss: 0.0031\nEpoch 106/200\n320/320 [==============================] - 0s 60us/step - loss: 0.0041 - val_loss: 0.0031\nEpoch 107/200\n320/320 [==============================] - 0s 49us/step - loss: 0.0041 - val_loss: 0.0031\nEpoch 108/200\n320/320 [==============================] - 0s 49us/step - loss: 0.0041 - val_loss: 0.0030\nEpoch 109/200\n320/320 [==============================] - 0s 51us/step - loss: 0.0041 - val_loss: 0.0030\nEpoch 110/200\n320/320 [==============================] - 0s 55us/step - loss: 0.0041 - val_loss: 0.0030\nEpoch 111/200\n320/320 [==============================] - 0s 53us/step - loss: 0.0041 - val_loss: 0.0030\nEpoch 112/200\n320/320 [==============================] - 0s 53us/step - loss: 0.0040 - val_loss: 0.0030\nEpoch 113/200\n320/320 [==============================] - 0s 50us/step - loss: 0.0040 - val_loss: 0.0030\nEpoch 114/200\n320/320 [==============================] - 0s 53us/step - loss: 0.0041 - val_loss: 0.0030\nEpoch 115/200\n320/320 [==============================] - 0s 58us/step - loss: 0.0040 - val_loss: 0.0030\nEpoch 116/200\n320/320 [==============================] - 0s 58us/step - loss: 0.0040 - val_loss: 0.0030\nEpoch 117/200\n320/320 [==============================] - 0s 49us/step - loss: 0.0040 - val_loss: 0.0030\nEpoch 118/200\n320/320 [==============================] - 0s 48us/step - loss: 0.0040 - val_loss: 0.0029\nEpoch 119/200\n320/320 [==============================] - 0s 47us/step - loss: 0.0041 - val_loss: 0.0030\nEpoch 120/200\n320/320 [==============================] - 0s 53us/step - loss: 0.0040 - val_loss: 0.0029\nEpoch 121/200\n320/320 [==============================] - 0s 55us/step - loss: 0.0040 - val_loss: 0.0030\nEpoch 122/200\n320/320 [==============================] - 0s 50us/step - loss: 0.0040 - val_loss: 0.0030\nEpoch 123/200\n320/320 [==============================] - 0s 52us/step - loss: 0.0040 - val_loss: 0.0030\nEpoch 124/200\n320/320 [==============================] - 0s 48us/step - loss: 0.0040 - val_loss: 0.0029\nEpoch 125/200\n320/320 [==============================] - 0s 51us/step - loss: 0.0040 - val_loss: 0.0029\nEpoch 126/200\n320/320 [==============================] - 0s 50us/step - loss: 0.0040 - val_loss: 0.0030\nEpoch 127/200\n320/320 [==============================] - 0s 63us/step - loss: 0.0040 - val_loss: 0.0029\nEpoch 128/200\n320/320 [==============================] - 0s 58us/step - loss: 0.0040 - val_loss: 0.0029\nEpoch 129/200\n320/320 [==============================] - 0s 53us/step - loss: 0.0040 - val_loss: 0.0030\nEpoch 130/200\n320/320 [==============================] - 0s 47us/step - loss: 0.0040 - val_loss: 0.0029\nEpoch 131/200\n320/320 [==============================] - 0s 58us/step - loss: 0.0040 - val_loss: 0.0029\nEpoch 132/200\n320/320 [==============================] - 0s 52us/step - loss: 0.0040 - val_loss: 0.0029\nEpoch 133/200\n320/320 [==============================] - 0s 49us/step - loss: 0.0040 - val_loss: 0.0029\nEpoch 134/200\n320/320 [==============================] - 0s 50us/step - loss: 0.0039 - val_loss: 0.0029\nEpoch 135/200\n320/320 [==============================] - 0s 49us/step - loss: 0.0039 - val_loss: 0.0029\nEpoch 136/200\n320/320 [==============================] - 0s 50us/step - loss: 0.0039 - val_loss: 0.0029\nEpoch 137/200\n320/320 [==============================] - 0s 49us/step - loss: 0.0039 - val_loss: 0.0028\nEpoch 138/200\n320/320 [==============================] - 0s 52us/step - loss: 0.0039 - val_loss: 0.0029\nEpoch 139/200\n320/320 [==============================] - 0s 49us/step - loss: 0.0039 - val_loss: 0.0028\nEpoch 140/200\n320/320 [==============================] - 0s 51us/step - loss: 0.0039 - val_loss: 0.0028\nEpoch 141/200\n320/320 [==============================] - 0s 53us/step - loss: 0.0039 - val_loss: 0.0028\nEpoch 142/200\n320/320 [==============================] - 0s 52us/step - loss: 0.0039 - val_loss: 0.0028\nEpoch 143/200\n320/320 [==============================] - 0s 52us/step - loss: 0.0039 - val_loss: 0.0029\nEpoch 144/200\n320/320 [==============================] - 0s 51us/step - loss: 0.0039 - val_loss: 0.0028\nEpoch 145/200\n320/320 [==============================] - 0s 52us/step - loss: 0.0039 - val_loss: 0.0028\nEpoch 146/200\n320/320 [==============================] - 0s 54us/step - loss: 0.0039 - val_loss: 0.0028\nEpoch 147/200\n320/320 [==============================] - 0s 53us/step - loss: 0.0039 - val_loss: 0.0028\nEpoch 148/200\n320/320 [==============================] - 0s 49us/step - loss: 0.0039 - val_loss: 0.0028\nEpoch 149/200\n320/320 [==============================] - 0s 52us/step - loss: 0.0039 - val_loss: 0.0028\nEpoch 150/200\n320/320 [==============================] - 0s 53us/step - loss: 0.0039 - val_loss: 0.0028\nEpoch 151/200\n320/320 [==============================] - 0s 53us/step - loss: 0.0039 - val_loss: 0.0028\nEpoch 152/200\n320/320 [==============================] - 0s 53us/step - loss: 0.0039 - val_loss: 0.0028\nEpoch 153/200\n320/320 [==============================] - 0s 52us/step - loss: 0.0039 - val_loss: 0.0028\nEpoch 154/200\n320/320 [==============================] - 0s 51us/step - loss: 0.0039 - val_loss: 0.0028\nEpoch 155/200\n320/320 [==============================] - 0s 51us/step - loss: 0.0039 - val_loss: 0.0028\nEpoch 156/200\n320/320 [==============================] - 0s 46us/step - loss: 0.0039 - val_loss: 0.0028\nEpoch 157/200\n320/320 [==============================] - 0s 48us/step - loss: 0.0039 - val_loss: 0.0027\nEpoch 158/200\n320/320 [==============================] - 0s 52us/step - loss: 0.0038 - val_loss: 0.0028\nEpoch 159/200\n320/320 [==============================] - 0s 52us/step - loss: 0.0039 - val_loss: 0.0027\nEpoch 160/200\n320/320 [==============================] - 0s 48us/step - loss: 0.0038 - val_loss: 0.0028\nEpoch 161/200\n320/320 [==============================] - 0s 51us/step - loss: 0.0039 - val_loss: 0.0028\nEpoch 162/200\n320/320 [==============================] - 0s 52us/step - loss: 0.0038 - val_loss: 0.0027\nEpoch 163/200\n320/320 [==============================] - 0s 50us/step - loss: 0.0038 - val_loss: 0.0027\nEpoch 164/200\n320/320 [==============================] - 0s 51us/step - loss: 0.0038 - val_loss: 0.0027\nEpoch 165/200\n320/320 [==============================] - 0s 50us/step - loss: 0.0038 - val_loss: 0.0027\nEpoch 166/200\n320/320 [==============================] - 0s 49us/step - loss: 0.0038 - val_loss: 0.0027\nEpoch 167/200\n320/320 [==============================] - 0s 49us/step - loss: 0.0038 - val_loss: 0.0027\nEpoch 168/200\n320/320 [==============================] - 0s 52us/step - loss: 0.0038 - val_loss: 0.0027\nEpoch 169/200\n320/320 [==============================] - 0s 51us/step - loss: 0.0038 - val_loss: 0.0027\nEpoch 170/200\n320/320 [==============================] - 0s 54us/step - loss: 0.0039 - val_loss: 0.0027\nEpoch 171/200\n320/320 [==============================] - 0s 52us/step - loss: 0.0038 - val_loss: 0.0028\nEpoch 172/200\n320/320 [==============================] - 0s 48us/step - loss: 0.0038 - val_loss: 0.0027\nEpoch 173/200\n320/320 [==============================] - 0s 49us/step - loss: 0.0038 - val_loss: 0.0027\nEpoch 174/200\n320/320 [==============================] - 0s 50us/step - loss: 0.0038 - val_loss: 0.0027\nEpoch 175/200\n320/320 [==============================] - 0s 52us/step - loss: 0.0039 - val_loss: 0.0027\nEpoch 176/200\n320/320 [==============================] - 0s 53us/step - loss: 0.0039 - val_loss: 0.0029\nEpoch 177/200\n320/320 [==============================] - 0s 55us/step - loss: 0.0038 - val_loss: 0.0026\nEpoch 178/200\n320/320 [==============================] - 0s 56us/step - loss: 0.0038 - val_loss: 0.0028\nEpoch 179/200\n320/320 [==============================] - 0s 54us/step - loss: 0.0038 - val_loss: 0.0026\nEpoch 180/200\n320/320 [==============================] - 0s 52us/step - loss: 0.0038 - val_loss: 0.0027\nEpoch 181/200\n320/320 [==============================] - 0s 56us/step - loss: 0.0038 - val_loss: 0.0027\nEpoch 182/200\n320/320 [==============================] - 0s 53us/step - loss: 0.0037 - val_loss: 0.0026\nEpoch 183/200\n320/320 [==============================] - 0s 52us/step - loss: 0.0038 - val_loss: 0.0026\nEpoch 184/200\n320/320 [==============================] - 0s 50us/step - loss: 0.0038 - val_loss: 0.0027\nEpoch 185/200\n320/320 [==============================] - 0s 50us/step - loss: 0.0038 - val_loss: 0.0026\nEpoch 186/200\n320/320 [==============================] - 0s 50us/step - loss: 0.0038 - val_loss: 0.0027\nEpoch 187/200\n320/320 [==============================] - 0s 49us/step - loss: 0.0038 - val_loss: 0.0027\nEpoch 188/200\n320/320 [==============================] - 0s 50us/step - loss: 0.0038 - val_loss: 0.0026\nEpoch 189/200\n320/320 [==============================] - 0s 50us/step - loss: 0.0038 - val_loss: 0.0027\nEpoch 190/200\n320/320 [==============================] - 0s 50us/step - loss: 0.0038 - val_loss: 0.0026\nEpoch 191/200\n320/320 [==============================] - 0s 49us/step - loss: 0.0038 - val_loss: 0.0027\nEpoch 192/200\n320/320 [==============================] - 0s 49us/step - loss: 0.0037 - val_loss: 0.0026\nEpoch 193/200\n320/320 [==============================] - 0s 53us/step - loss: 0.0038 - val_loss: 0.0026\nEpoch 194/200\n320/320 [==============================] - 0s 50us/step - loss: 0.0037 - val_loss: 0.0027\nEpoch 195/200\n320/320 [==============================] - 0s 55us/step - loss: 0.0038 - val_loss: 0.0026\nEpoch 196/200\n320/320 [==============================] - 0s 56us/step - loss: 0.0038 - val_loss: 0.0027\nEpoch 197/200\n320/320 [==============================] - 0s 54us/step - loss: 0.0039 - val_loss: 0.0026\nEpoch 198/200\n320/320 [==============================] - 0s 55us/step - loss: 0.0037 - val_loss: 0.0027\nEpoch 199/200\n320/320 [==============================] - 0s 51us/step - loss: 0.0038 - val_loss: 0.0026\nEpoch 200/200\n320/320 [==============================] - 0s 47us/step - loss: 0.0037 - val_loss: 0.0026\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"y_pred = model.predict(X_test_scaled)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T03:56:48.372683Z","iopub.execute_input":"2025-08-24T03:56:48.372999Z","iopub.status.idle":"2025-08-24T03:56:48.407606Z","shell.execute_reply.started":"2025-08-24T03:56:48.372948Z","shell.execute_reply":"2025-08-24T03:56:48.406658Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"from sklearn.metrics import r2_score\nr2_score(y_test, y_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T03:56:49.900496Z","iopub.execute_input":"2025-08-24T03:56:49.900838Z","iopub.status.idle":"2025-08-24T03:56:49.908101Z","shell.execute_reply.started":"2025-08-24T03:56:49.900786Z","shell.execute_reply":"2025-08-24T03:56:49.907024Z"}},"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"0.8083387459207302"},"metadata":{}}],"execution_count":53},{"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T03:57:42.970190Z","iopub.execute_input":"2025-08-24T03:57:42.970552Z","iopub.status.idle":"2025-08-24T03:57:43.269051Z","shell.execute_reply.started":"2025-08-24T03:57:42.970495Z","shell.execute_reply":"2025-08-24T03:57:43.267969Z"}},"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"[<matplotlib.lines.Line2D at 0x7aa696c77518>]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}],"execution_count":54}]}